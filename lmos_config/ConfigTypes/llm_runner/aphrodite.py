from typing import List, Optional, Literal
from ..generic.service import InternalService
from pydantic import BaseModel, ConfigDict


class AphroditeArgs(BaseModel):
    # mostly generated by chatGPT
    uvicorn_log_level: Optional[
        Literal["debug", "info", "warning", "error", "critical", "trace"]
    ]
    lora_modules: Optional[List[str]]
    prompt_adapters: Optional[List[str]]
    chat_template: Optional[str]
    response_role: Optional[str]
    root_path: Optional[str]
    middleware: Optional[List[str]]
    launch_kobold_api: Optional[bool]
    max_log_len: Optional[int]
    return_tokens_as_token_ids: Optional[bool]
    disable_frontend_multiprocessing: Optional[bool]
    seed: Optional[int]
    served_model_name: Optional[List[str]]
    tokenizer: Optional[str]
    revision: Optional[str]
    code_revision: Optional[str]
    tokenizer_revision: Optional[str]
    tokenizer_mode: Optional[Literal["auto", "slow"]]
    trust_remote_code: Optional[bool]
    download_dir: Optional[str]
    max_model_len: Optional[int]
    max_context_len_to_capture: Optional[int]
    max_seq_len_to_capture: Optional[int]
    rope_scaling: Optional[str]
    rope_theta: Optional[float]
    model_loader_extra_config: Optional[str]
    enforce_eager: Optional[bool]
    skip_tokenizer_init: Optional[bool]
    tokenizer_pool_size: Optional[int]
    tokenizer_pool_type: Optional[str]
    tokenizer_pool_extra_config: Optional[str]
    max_logprobs: Optional[int]
    device: Optional[Literal["auto", "cuda", "neuron", "cpu", "openvino", "tpu", "xpu"]]
    load_format: Optional[
        Literal[
            "auto",
            "pt",
            "safetensors",
            "npcache",
            "dummy",
            "tensorizer",
            "sharded_state",
            "bitsandbytes",
        ]
    ]
    dtype: Optional[Literal["auto", "half", "float16", "bfloat16", "float", "float32"]]
    ignore_patterns: Optional[str]
    worker_use_ray: Optional[bool]
    tensor_parallel_size: Optional[int]
    pipeline_parallel_size: Optional[int]
    ray_workers_use_nsight: Optional[bool]
    disable_custom_all_reduce: Optional[bool]
    distributed_executor_backend: Optional[Literal["ray", "mp"]]
    max_parallel_loading_workers: Optional[int]
    quantization: Optional[
        Literal[
            "aqlm",
            "awq",
            "deepspeedfp",
            "eetq",
            "fp8",
            "fbgemm_fp8",
            "gguf",
            "marlin",
            "gptq_marlin_24",
            "gptq_marlin",
            "awq_marlin",
            "gptq",
            "quip",
            "squeezellm",
            "compressed-tensors",
            "bitsandbytes",
            "qqq",
            None,
        ]
    ]
    quantization_param_path: Optional[str]
    preemption_mode: Optional[str]
    deepspeed_fp_bits: Optional[int]
    kv_cache_dtype: Optional[Literal["auto", "fp8", "fp8_e5m2", "fp8_e4m3"]]
    block_size: Optional[Literal[8, 16, 32]]
    enable_prefix_caching: Optional[bool]
    num_gpu_blocks_override: Optional[int]
    disable_sliding_window: Optional[bool]
    gpu_memory_utilization: Optional[float]
    swap_space: Optional[int]
    cpu_offload_gb: Optional[int]
    use_v2_block_manager: Optional[bool]
    scheduler_delay_factor: Optional[float]
    enable_chunked_prefill: Optional[bool]
    guided_decoding_backend: Optional[Literal["outlines", "lm-format-enforcer"]]
    max_num_batched_tokens: Optional[int]
    max_num_seqs: Optional[int]
    num_lookahead_slots: Optional[int]
    speculative_model: Optional[str]
    num_speculative_tokens: Optional[int]
    speculative_max_model_len: Optional[int]
    ngram_prompt_lookup_max: Optional[int]
    ngram_prompt_lookup_min: Optional[int]
    speculative_draft_tensor_parallel_size: Optional[int]
    speculative_disable_by_batch_size: Optional[int]
    spec_decoding_acceptance_method: Optional[
        Literal["rejection_sampler", "typical_acceptance_sampler"]
    ]
    typical_acceptance_sampler_posterior_threshold: Optional[float]
    typical_acceptance_sampler_posterior_alpha: Optional[float]
    disable_logprobs_during_spec_decoding: Optional[bool]
    enable_lora: Optional[bool]
    max_loras: Optional[int]
    max_lora_rank: Optional[int]
    lora_extra_vocab_size: Optional[int]
    lora_dtype: Optional[Literal["auto", "float16", "bfloat16", "float32"]]
    max_cpu_loras: Optional[int]
    long_lora_scaling_factors: Optional[str]
    fully_sharded_loras: Optional[bool]
    qlora_adapter_name_or_path: Optional[str]
    enable_prompt_adapter: Optional[bool]
    max_prompt_adapters: Optional[int]
    max_prompt_adapter_token: Optional[int]
    disable_log_stats: Optional[bool]
    engine_use_ray: Optional[bool]
    disable_log_requests: Optional[bool]
    uvloop: Optional[bool]

    model_config = ConfigDict(protected_namespaces=())


class AphroditeRunner(InternalService, AphroditeArgs):
    """ExllamaV2 runner config"""

    type: Literal["aphrodite"]
    _port: int = 8000
